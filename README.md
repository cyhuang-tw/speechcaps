# SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning

This is the official imeplementation of the paper "SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning".
We introduce a multi-talker speaking style captioning task to enhance the understanding of speaker and prosodic information.
We generate synthetic multi-talker audio using [PromptSpeech](https://arxiv.org/abs/2211.12171) and leverage ChatGPT to derive speaking style descriptions from readily available metadata.

## Data
You can access the training data on [Hugging Face](https://huggingface.co/datasets/cyhuang-tw/PromptTTS-train-vad-v2-adjusted-tag), which includes attribute annotations for a subset of utterances from PromptSpeech.
For the testing data, please refer to this [link]((https://huggingface.co/datasets/cyhuang-tw/PromptTTS-test-vad-v2-adjusted-tag)). The natural language descriptions generated by the LLM will be released soon (expected by the end of April).
If you have any questions about the paper or dataset, feel free to open an issue.



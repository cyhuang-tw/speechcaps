# SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning

This is the official imeplementation of the paper "SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning".
We introduce a multi-talker speaking style captioning task to enhance the understanding of speaker and prosodic information.
We generate synthetic multi-talker audio using [PromptSpeech](https://arxiv.org/abs/2211.12171) and leverage ChatGPT to derive speaking style descriptions from readily available metadata.

Currently, the data is in the preparation phase, and we plan to release it on Huggingface to facilitate easy access for researchers.
